title: "Practical Machine Learning - Course Project"
author: "Tom Kaier"
date: "07/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document outlines the machine learning analysis performed on 
a data set of exercise routines collected by a wearable device.

The goal of the analysis is to devise a model which will predict
the manner in which a particular set of exercises was done.

## Overview of Training and Test Data

The traing data is available 
here: [Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pmltraining.csv).

The test data is available
here: [Test Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pmltesting.csv).

We can see a histogram of the outcome variable we're interested in: `classe`. It
has a roughly even distribution of values.

```{r, message=FALSE, results='hide'}
library(caret)
library(rattle)
training <- read.csv("./data/pml-training.csv")
testing <- read.csv("./data/pml-testing.csv")
# The outcome we're interested in
plot(training$classe)
```

The training data consists of over 19K observations of 160 features. A quick look at the training data shows up a lot of columns where the values are mostly NA or empty. We can see this using the following code:

```{r}
# For each column sum up the number of NA or empty values
missing_data_sums <- colSums(is.na(training) | training == "") 
hist(missing_data_sums)
empty_cols <- names(missing_data_sums[missing_data_sums > 19000])
```

There are about such 100 columns. The histogram shows that there is a big break in the number of missing columns - in other words either a column has a lot of data or has very little data, but nothing inbetween. Let's remove these columns from our training set:

```{r}
training_2 <- training[, !(colnames(training) %in% empty_cols)]
```

This leaves us with 60 columns that are mostly populated. One variable I'd like to take out is "X". It's a row number of some sort:

```{r}
training_2 <- training_2[ , -which(names(training_2) %in% c("X"))]
```

Finally, let's shuffle the data to ensure that any previous ordering doesn't affect our results:

```{r}
training_2 <- training_2[sample(nrow(training_2 )), ]
```

## Training Data Set and Cross Validation Data Set

The training data set still has a lot of rows. We will split into training (n=1000), and cross-validation set (n=1000).

```{r}
# Subset of this for actual training 
training_3 <- training_2[1:2000,]
# Take the next 1000 entries for cross-validation
cross_val <- training_2[2001:3000,]
```

Once we've seen some initial results, we will increase the size of the training
data set to see how accuracy improves.

## Machine Learning Algorithms Used

We try four different ML methods with different training set sizes:

* Linear Discriminant Analysis - LDA
* Classification Trees - Rpart
* Random Forests - RF
* Generalized Boosted Regression Models - GBM

Random Forests and GBM were chosen in particular because they handle data sets with a large number of features very well.For each method we measure the accuracy of the modelin the cross-valiation data set.

## Code for each run

The code for each run is exactly the same. Only the size of the training set is different. 
Simple timings using `Sys.time()` are used to show how long each run took.

### LDA

```{r, eval=FALSE, warning=FALSE}
# LDA
modelLDA <- train(classe ~ ., method="lda", data=training_3)
predsLDA <- predict(modelLDA, newdata = cross_val)
crosstabLDA <- table(predsLDA, cross_val$classe)
accuracyLDA <- (sum(diag(crosstabLDA)) * 100)/1000
predict(modelLDA, newdata = testing)
```

### Rpart

```{r, eval=FALSE, warning=FALSE}
modelRpart <- train(classe ~ ., method="rpart", data=training_3)
predsRpart <- predict(modelRpart, newdata = cross_val)
crosstabRpart <- table(predsRpart, cross_val$classe)
accuracyRpart <- (sum(diag(crosstabRpart)) * 100)/1000
predict(modelRpart, newdata = testing)
```

### RF

```{r, eval=FALSE, warning=FALSE}
modelRF <- train(classe ~ ., method="rf", data=training_3)
predsRF <- predict(modelRF, newdata = cross_val)
crosstabRF <- table(predsRF, cross_val$classe)
accuracyRF <- (sum(diag(crosstabRF)) * 100)/1000
predict(modelRF, newdata = testing)
```

### GBM

```{r, eval=FALSE, warning=FALSE}
modelGBM <- train(classe ~ ., method="gbm", data=training_3, verbose=FALSE)
predsGBM <- predict(modelGBM, newdata = cross_val)
crosstabGBM <- table(predsGBM, cross_val$classe)
accuracyGBM <- (sum(diag(crosstabGBM)) * 100)/1000
predict(modelGBM, newdata = testing)
```

## Accuracy Results

For each machine learning method we plot the accuracy (in %) against
the different training data set sizes.

Method vs N | 1000 | 2000 | 5000 | 10000
----------- | ---- | ---- | ---- | ----   
LDA         | 83.8 | 85.7 | 83.9 | 86.7
RPart       | 55.4 | 55.7 | 55.5 | 61.9
RF          | 97.4 | 99.2 | 99.5 | 100
GBM         | 97.0 | 98.7 | 99.5 | 99.9

The RPart results did not impove substantially by increasing N; the other approaches improve all slightly in terms of accuracy, and - other than RF - don't require that much longer to compute.

## Prediction Results

For each machine learing method we show the preditions on the
test set for each training data set size.

### LDA

N     | Prediction
----- | ---------------------------------------
1000  | C A B A A D D B A A B C B A E E A B B B
2000  | B B B A A E D C A A B C B A E E A B A B
5000  | B B B A A E D C A A B C B A E E A B B B
10000 | B B B A A E D C A A B C B A E E A B B B        

The predictions are relatively consistent with increasing N.

### Rpart

N     | Prediction
----- | ---------------------------------------
1000  | B A A C A C C C A A B C B A C B B B B B
2000  | B A A A A C C C A A B C B A C B B B B B
5000  | C C A A A D C D A A D C B A C C C D A B
10000 | C D A A A C D D A A C D B A D D C D A B

The predictions look odd - hardly any Ds or Es.


### RF

N     | Prediction
----- | ---------------------------------------
1000  | B A A A A E D B A A B C B A E E A B B B
2000  | B A A A A E D B A A B C B A E E A B B B
5000  | B A A A A E D B A A B C B A E E A B B B
10000 | B A B A A E D B A A B C B A E E A B B B

The predictions are very consistent, even with N relatively
small.

### GBM

N     | Prediction
----- | ---------------------------------------
1000  | B A A A A E D B A A B C B A E E A B B B
2000  | B A B A A E D B A A B C B A E E A B B B
5000  | B A B A A E D B A A B C B A E E A B B B
10000 | B A B A A E D B A A B C B A E E A B B B

Again, the predictions here are very consistent, even with N relatively
small.

### Conclusion

The RF and GBM predictions all agree with each other, regardless of 
the choice of N, so we could conclude that setting N=1000 is good enough
for our purposes using these algorithms. For LDA, you need a much higher
value of N to get the predictions approaching the accuracy of the others.
Rpart remains sub-par throughout.

From a performance point of view, the Random Forests algorithm is the slowest
to run, while the GBM one is about twice as fast as RF, even though its 
accuracy is about the same as RF.

We conclude that the use of a Generalised Boosted Model technique, as implemented using 
the GBM R package, works very well for accurately predicting how well the subjects
trained based on the training data provided.